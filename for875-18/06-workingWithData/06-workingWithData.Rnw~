\chapter{Working with Data}\label{CHAPTER:DATA2}
Bringing data into R, exporting data from R in a form that is readable by other software, cleaning and reshaping data, and other data manipulation tasks are an important and often overlooked component of data science. The book \citet{SpectorDataManipulation}, while a few years old, is still an excellent reference for data-related issues. And the \emph{R Data Import/Export} manual, available online at \url{https://cran.r-project.org/doc/manuals/r-release/R-data.html}, is an up-to-date (and free) reference on importing a wide variety of datasets into R and on exporting data in various forms.

\section{Reading data into R}
Data come in a dizzying variety of forms. It might be in a proprietary format such as an \href{https://en.wikipedia.org/wiki/Microsoft\_Excel}{.xlsx} Excel file, a \href{https://en.wikipedia.org/wiki/SPSS}{.sav} SPSS file, or a \href{https://en.wikipedia.org/wiki/Minitab}{.mtw} Minitab file. It might be structured using a \href{https://en.wikipedia.org/wiki/Relational_model}{relational model} comprising many tables that need to be connected via key-value pairs. It might be a data-interchange format such as \href{http://www.json.org/}{JSON} (JavaScript Object Notation),  or a markup language such as \href{https://en.wikipedia.org/wiki/XML}{XML} (Extensible Markup Language), perhaps with specialized standards for describing ecological information, see \href{https://en.wikipedia.org/wiki/Ecological_Metadata_Language}{EML} (Ecological Metadata Language). Both XML and EML are common data metadata formats (i.e., data that provides information about other data). Fortunately many datasets are (or can be) saved as plain text files, and most software can both read and write such files, so our initial focus will be on reading plain text files into R and saving data from R in plain text format. RStudio provides a handy \href{http://www.rstudio.com/resources/cheatsheets}{data import cheat sheet} for many of the read functions detailed in this section.

The \verb+foreign+ R package provides functions to directly read data saved in some of the proprietary formats into R, which is sometimes unavoidable, but if possible it is good to save data from  another package as plain text and then read this plain text file into R. In Chapter~\ref{CHAPTER:XML} methods for reading web-based data sets into R will be discussed.

The function \verb+read.table()+ and its offshoots such as \verb+read.csv()+ are used to read in rectangular data from a text file. For example, the file \emph{BrainAndBody.csv} contains data\footnote{These data come from the \texttt{MASS} R library.} on the brain weight, body weight, and name of some terrestrial animals. Here are the first few lines of that file:
\begin{verbatim}
body,brain,name
1.35,8.1,Mountain beaver
465,423,Cow
36.33,119.5,Grey wolf
27.66,115,Goat
1.04,5.5,Guinea pig
\end{verbatim}
As is evident, the first line of the file contains the names of the three variables, separated (delimited) by commas. Each subsequent line contains the body weight, brain weight, and name of a specific terrestrial animal.

This file is accessible at the url \url{http://blue.for.msu.edu/FOR875/data/BrainAndBody.csv}. The \verb+read.table()+ function is used to read these data into an R data frame.
<<tidy=FALSE>>=
u.bb <- "http://blue.for.msu.edu/FOR875/data/BrainAndBody.csv"
BrainBody <- read.table(file = u.bb, header = TRUE, sep = ",", 
                        stringsAsFactors = FALSE)
head(BrainBody)
@
The arguments used in this call to \verb+read.table()+ include:
\begin{enumerate}
\item \verb+file = u.bb+, which tells R the location of the file. In this case the string \url{http://blue.for.msu.edu/FOR875/data/BrainAndBody.csv} giving the location is rather long, so it was first assigned to the object \verb+u.bb+.
\item \verb+header = TRUE+, which tells R the first line of the file gives the names of the variables.
\item \verb+sep = ","+, which tells R that a comma separates the fields in the file.
\item \verb+stringsAsFactors = FALSE+ which tells R not to convert character vectors to factors.
\end{enumerate}
The function \verb+read.csv()+ is the same as \verb+read.table()+ except the default separator is a comma, whereas the default separator for \verb+read.table()+ is whitespace. 

The file \url{BrainAndBody.tsv} contains the same data, except a tab is used in place of a comma to separate fields. The only change needed to read in the data in this file is in the \verb+sep+ argument (and of course the \verb+file+ argument, since the data are stored in a different file):
<<tidy=FALSE>>=
u.bb <- "http://blue.for.msu.edu/FOR875/data/BrainAndBody.tsv"
BrainBody2 <- read.table(file = u.bb, header = TRUE, sep = "\t", 
                         stringsAsFactors = FALSE)
head(BrainBody2)
@
File extensions, e.g., \verb+.csv+ or \verb+.tsv+, are naming conventions only and are there to remind us how the columns are separated. In other words, they have no influence on R's file read functions.

A third file, \url{BrainAndBody.txt}, contains the same data, but also contains a few lines of explanatory text above the names of the variables. It also uses whitespace rather than a comma or a tab as a separator. Here are the first several lines of the file.
\begin{verbatim}
This file contains data
on brain and body
weights of several terrestrial animals

"body" "brain" "name"
1.35 8.1 "Mountain beaver"
465 423 "Cow"
36.33 119.5 "Grey wolf"
27.66 115 "Goat"
1.04 5.5 "Guinea pig"
11700 50 "Dipliodocus"
2547 4603 "Asian elephant"
\end{verbatim}
Notice that in this file the values of \verb+name+ are put inside of quotation marks. This is necessary since instead R would (reasonably) assume the first line contained the values of four variables, the values being \verb+1.35+, \verb+8.1+, \verb+Mountain+, and \verb+beaver+ while in reality there are only three values desired, with \verb+Mountain beaver+ being the third. 

To read in this file we need to tell R to skip the first five lines and to use whitespace as the separator. The \verb+skip+ argument handles the first, and the \verb+sep+ argument the second. First let's see what happens if we don't use the \verb+skip+ argument.
<<tidy=FALSE>>=
u.bb <- "http://blue.for.msu.edu/FOR875/data/BrainAndBody.txt"
BrainBody3 <- read.table(u.bb, header = TRUE, sep = " ", 
                         stringsAsFactors = FALSE)
@
R assumed the first line of the file contained the variable names, since \verb+header = TRUE+ was specified, and counted four including \verb+This+, \verb+file+, \verb+contains+, and \verb+data+. So in the first line of actual data, R expected four columns containing data plus possibly a fifth column containing row names for the data set, and complained that ``line 1 did not have 5 elements." The error message is somewhat mysterious, since it starts with ``Error in scan.'' This happens because \verb+read.table()+ actually uses a more basic R function called \verb+scan()+ to do the work. 

Here's how to read in the file correctly.
<<tidy=FALSE>>=
u.bb <- "http://blue.for.msu.edu/FOR875/data/BrainAndBody.txt"
BrainBody3 <- read.table(u.bb, header = TRUE, sep = " ", 
                         stringsAsFactors = FALSE, skip = 4)
BrainBody3[1:10,]
@

\subsection{Reading data with missing observations}
Missing data is represented in many ways. Sometimes missing data is just that, i.e., the place where it should be in the file is blank. Other times specific numbers such as $-9999$ or specific symbols are used. The \verb+read.table()+ function has an argument \verb+na.string+ that allows the user to specify how missing data is indicated in the source file.

The site \url{http://www.wunderground.com/history/} makes weather data available for locations around the world from dates going back to 1945. The file \url{WeatherKLAN2014.csv} contains weather data for Lansing, Michigan for the year 2014. Here are the first few lines of that file:
\begin{verbatim}
EST,Max TemperatureF,Min TemperatureF, Events
1/1/14,14,9,Snow
1/2/14,13,-3,Snow
1/3/14,13,-11,Snow
1/4/14,31,13,Snow
1/5/14,29,16,Fog-Snow
1/6/14,16,-12,Fog-Snow
1/7/14,2,-13,Snow
1/8/14,17,-1,Snow
1/9/14,21,2,Snow
1/10/14,39,21,Fog-Rain-Snow
1/11/14,41,32,Fog-Rain
1/12/14,39,31,
\end{verbatim}
Look at the last line, and notice that instead of an \verb+Event+ such as \verb+Snow+ or \verb+Fog-Snow+ there is nothing after the comma. This observation is missing, but rather than using an explicit code such as \verb+NA+, the site just leaves that entry blank. To read these data into R we will supply the argument \verb+na.string = ""+ which tells R the file indicates missing data by leaving the appropriate entry blank.
<<tidy=FALSE>>=
u.weather <- "http://blue.for.msu.edu/FOR875/data/WeatherKLAN2014.csv"
WeatherKLAN2014 <- read.csv(u.weather, header=TRUE, 
                            stringsAsFactors = FALSE, na.string = "")
WeatherKLAN2014[1:15,]
@

\section{Summarizing data frames}
Some common data tasks include variable summaries such as means or standard deviations, transforming an existing variable, and creating new variables. As with many tasks, there are several ways to accomplish each of these.

\subsection{Column (and row) summaries}
The file \url{WeatherKLAN2014Full.csv} contains a more complete set of weather data variables than \url{WeatherKLAN2014.csv}, from the same source, \url{http://www.wunderground.com/history}. 
<<tidy=FALSE>>=
u.weather <- "http://blue.for.msu.edu/FOR875/data/WeatherKLAN2014Full.csv"
WeatherKLAN2014Full <- read.csv(u.weather, header=TRUE, 
                               stringsAsFactors = FALSE, 
                               na.string = "")
names(WeatherKLAN2014Full)
@
How can we compute the mean for each variable? One possibility is to do this a variable at a time:
<<>>=
mean(WeatherKLAN2014Full$Mean.TemperatureF)
mean(WeatherKLAN2014Full$Min.TemperatureF)
mean(WeatherKLAN2014Full$Max.TemperatureF)
##Et Cetera
@
This is pretty inefficient. Fortunately there is a \verb+colMeans()+ function which computes the mean of each column (or a specified number of columns) in a data frame. Some columns in the current data frame are not numeric, and obviously we don't want to ask R to compute means for these columns. We use \verb+str()+ to investigate.
<<>>=
str(WeatherKLAN2014Full)
@
It isn't surprising that \verb+EST+ and \verb+Events+ are not numeric, but is surprising that \verb+PrecipitationIn+, which measures precipitation in inches, also is not numeric, but is character. Let's investigate further.
<<>>=
WeatherKLAN2014Full$PrecipitationIn[1:50]
@
Now it's more clear. The original data file included \verb+T+ in the precipitation column to represent a ``trace'' of precipitation, which is precipitation greater than $0$ but less than $0.01$ inches. One possibility would be to set all these values to \verb+"0"+, and then to convert the column to numeric. For now we will just leave the \verb+PrecipitationIn+ column out of the columns for which we request the mean.
<<>>=
colMeans(WeatherKLAN2014Full[,c(2:19, 21, 23)])
@

\subsection{The \texttt{apply()} function}\label{SEC:APPLY}
R also has functions \verb+rowMeans()+, \verb+colSums()+, and \verb+rowSums()+. But what if we want to compute the median or standard deviation of columns of data, or some other summary statistic? For this the \verb+apply()+ function can be used. This function applies a user-chosen function to either the rows or columns (or both) of a data frame. The arguments are:
\be
\item \verb+X+: the data frame of interest
\item \verb+MARGIN+: specifying either rows (\verb+MARGIN = 1+) or columns (\verb+MARGIN = 2+)
\item \verb+FUN+: the function to be applied.
\ee
<<>>=
apply(X = WeatherKLAN2014Full[,c(2:19, 21, 23)], MARGIN = 2, FUN = sd)
@
As with any R function the arguments don't need to be named as long as they are specified in the correct order, so 
\begin{verbatim}
apply(WeatherKLAN2014Full[,c(2:19, 21, 23)], 2, sd)
\end{verbatim}
has the same result\footnote{See if you can figure out why \texttt{Max.Gust.SpeedMPH} is \texttt{NA} in the \texttt{apply()} output and make it return a numeric value. Hint, take a look at the \texttt{sd()} definition}.

\subsection{Saving typing using \texttt{with()}}
Consider calculating the mean of the maximum temperature values for those days where the cloud cover is less than 4 and when the maximum humidity is over 85. We can do this using subsetting.
<<tidy=FALSE>>=
mean(WeatherKLAN2014Full$Max.TemperatureF[
                             WeatherKLAN2014Full$CloudCover < 4 & 
                             WeatherKLAN2014Full$Max.Humidity > 85])
@
While this works, it requires a lot of typing, since each time we refer to a variable in the data set we need to preface its name by \verb+WeatherKLAN2014Full$+. The \verb+with()+ function tells R that we are working with a particular data frame, and we don't need to keep typing the name of the data frame.
<<>>=
with(WeatherKLAN2014Full, 
     mean(Max.TemperatureF[CloudCover < 4 & Max.Humidity > 85]))
@

\section{Transforming a data frame}
Commonly variables are added to, removed from, changed in, or rearranged in a data frame. The subsetting features of R make this reasonably easy. We will investigate this in the context of the \verb+gapminder+ data frame. If the \verb+gapminder+ library is not  yet installed, use \verb+install.packages("gapminder")+ to install it locally.

<<>>=
library(gapminder)
str(gapminder)
@

\subsection{Adding variables}
The data frame contains per capita GDP and population, and it might be interesting to create a variable that gives the total GDP by multiplying these two variables. (If we were interested in an accurate value for the total GDP we would probably be better off getting this information directly, since it is likely that the per capita GDP values in the data frame are rounded substantially.) 

<<>>=
gapminder$TotalGDP <- gapminder$gdpPercap * gapminder$pop
str(gapminder)
@

Analogous to the \verb+with()+ function, there is a function \verb+within()+ which can simplify the syntax. Whereas \verb+with()+ does not change the data frame, \verb+within()+ can. Note, below I first remove the altered gapminder dataframe using \verb+rm()+ then bring a clean copy back in by reloading the \verb+gapminder+ package.

<<>>=
rm(gapminder)
library(gapminder)
str(gapminder)
gapminder <- within(gapminder, TotalGDP <- gdpPercap * pop)
str(gapminder)
@

A nice feature of \verb+within()+ is its ability to add more than one variable at a time to a data frame. In this case the two or more formulas creating new variables must be enclosed in braces.
<<tidy=FALSE>>=
gapminder <- within(gapminder, {TotalGDP <- gdpPercap * pop
    lifeExpMonths <- lifeExp * 12})
str(gapminder)
@

\subsection{Removing variables}
After reflection we may realize the new variables we added to the \verb+gapminder+ data frame are not useful, and should be removed.
<<>>=
str(gapminder)
gapminder <- gapminder[1:6]
str(gapminder)
@
The same result could be obtained via \verb+gapminder <- gapminder[, 1:6]+. The first method uses the fact that a data frame is also a list, and uses list subsetting methods. It is slightly preferable, since even if only one variable is retained, the object will still be a data frame, while the other method can return a vector in this case. Note this difference in the resulting \verb+x+ variable below (again this behavior can be frustrating at times if it is not anticipated).
<<>>=
a <- data.frame(x = 1:3, y = c("dog", "cat", "pig"), 
                z = seq(from = 1, to = 2, length = 3))
a
a <- a[1]
a 
a <- data.frame(x = 1:3, y = c("dog", "cat", "pig"), 
                z = seq(from = 1, to = 2, length = 3))
a 
a <- a[,1]
a
@

One can also use a negative sign in front of the variable number(s). For example, \verb+a[-(2:3)]+ would drop the first two columns of \verb+a+. Some care is needed when removing variables using the negative sign---what happens if you write \verb+a[-2:3]+ instead of \verb+a[-(2:3)]+ (why are the parentheses important here)?

An alternative approach is to set the variables you'd like to remove to \verb+NULL+. For example, \verb+a[c("y","z")] <- NULL+ and \verb+a[,2:3] <- NULL+ produce the same result as above.

\subsection{Transforming variables}
Consider the gapminder data again. Possibly we don't want to add a new variable that gives life expectancy in months, but rather want to modify the existing variable to measure life expectancy in months. Here are two ways to accomplish this.
<<>>=
rm(gapminder)
library(gapminder)
gapminder$lifeExp[1:5]
gapminder$lifeExp <- gapminder$lifeExp * 12
gapminder$lifeExp[1:5]
rm(gapminder)
library(gapminder)
gapminder$lifeExp[1:5]
gapminder <- within(gapminder, lifeExp <- lifeExp * 12)
gapminder$lifeExp[1:5]
@

\subsection{Rearranging variables}
Consider the full weather data set again. 
<<tidy=FALSE>>=
u.weather <- "http://blue.for.msu.edu/FOR875/data/WeatherKLAN2014Full.csv"
WeatherKLAN2014Full <- read.csv(u.weather, header=TRUE, 
                                stringsAsFactors = FALSE,
                                na.string = "")
names(WeatherKLAN2014Full)
@
If we want the wind speed variables to come right after the date, we can again use subsetting.
<<>>=
WeatherKLAN2014Full <- WeatherKLAN2014Full[c(1,17, 18, 19, 2:16, 20:23)]
names(WeatherKLAN2014Full)
@

\section{Reshaping data}
A data set can be represented in several different formats. Consider a (fictitious) data set on incomes of three people during three different years. Here is one representation of the data:
<<echo = F>>=
yearlyIncomeWide <- data.frame(name = c("John Smith", "Jane Doe", "Albert Jones"), income1990 = c(29784, 56789, 2341), income2000 = c(39210, 89321, 34567), income2010 = c(41213, 109321, 56781))
library(tidyr)
yearlyIncomeLong <- gather(yearlyIncomeWide, year, income, income1990:income2010)
@
<<>>=
yearlyIncomeWide
@
Here is another representation of the same data:
<<>>=
yearlyIncomeLong
@
For hopefully obvious reasons, the first representation is called a \emph{wide} representation of the data, and the second is called a \emph{long} representation. Each has its merits. The first representation is probably easier for people to read, while the second is often the form needed for analysis by statistical software such as R. There are of course other representations. For example the rows and columns could be interchanged to create a different wide representation, or the long representation, which currently groups data by year, could group by name instead. 

Whatever the relative merits of wide and long representations of data, transforming data from wide to long or long to wide is often required. As with many tasks, there are several ways to accomplish this in R. We will focus on a relatively new library called \verb+tidyr+ written by Hadley Wickham that performs the transformations and more.

\subsection{Tidyr}
The R library \verb+tidyr+ has functions for converting data between formats. To illustrate its use, we examine a simple data set that explores the relationship between religion and income in the United States. The data come from a Pew survey, and are used in the \verb+tidyr+ documentation to illustrate transforming data from wide to long format. 

<<>>=
u.rel <- "http://blue.for.msu.edu/FOR875/data/religion2.csv"
religion <- read.csv(u.rel, header=TRUE, stringsAsFactors = FALSE)
head(religion)
@

As given, the columns include religion and income level, and there are counts for each of the combinations of religion and income level. For example, there are 27 people who are Agnostic and whose income is less than 10 thousand dollars, and there are 617 people who are Catholic and whose income is between 10 and 20 thousand dollars.

The \verb+gather()+ function can transform data from wide to long format. 

<<tidy=FALSE>>=
library(tidyr)
religionLong <- gather(data = religion, key = IncomeLevel, 
                       value = Frequency, 2:11)
head(religionLong)
tail(religionLong)
@
To use \verb+gather()+ we specified the data frame (\verb+data = religion+), the name we want to give to the column created from the income levels (\verb+key = IncomeLevel+), the name we want to give to the column containing the frequency values (\verb+value = Frequency+) and the columns to gather (\verb+2:11+). 

Columns to be gathered can be specified by name also, and we can also specify which columns should be omitted using a negative sign in front of the name(s). So the following creates an equivalent data frame:

<<tidy=FALSE>>=
religionLong <- gather(data = religion, key = IncomeLevel, 
                       value = Frequency, -religion)
head(religionLong)
@

<<>>=
religionWide <- spread(data = religionLong, key = IncomeLevel, 
                       value = Frequency)
head(religionWide)
@
Here we specify the data frame (\verb+religionLong+), the column (\verb+IncomeLevel+) to be spread, and the column of values (\verb+Frequency+) to be spread among the newly created columns. As can be seen, this particular call to \verb+spread()+ yields the original data frame.

\verb+tidyr+ provides two other useful functions to separate and unite variables based on some deliminator. Consider again the \verb+yearlyIncomeWide+ table. Say we want to split the \verb+name+ variable into first and last name. This can be done using the \verb+separate()+ function.
<<tidy=FALSE>>=
firstLast <- separate(data = yearlyIncomeLong, col = name, 
                      into = c("first", "last"), sep="\\s")
print(firstLast)
@ 
Now say, you're not happy with that and you want to combine the name column again, but this time separate the first and last name with a underscore. This is done using the \verb+unite()+ function.
<<>>=
unite(firstLast, col=name, first, last, sep="_")
@ 

\section{Manipulating data with \texttt{dplyr}}\label{sec:dplyr}
Much of the effort (a figure of 80\% is sometimes suggested) in data analysis is spent cleaning the data and getting it ready for analysis. Having effective tools for this task can save substantial time and effort. The R package \verb+dplyr+ written by Hadley Wickham is designed, in Hadley's words, to be ``a grammar of data manipulation, providing a consistent set of verbs that help you solve the most common data manipulation challenges.'' Casting data analysis tasks in terms of ``grammar'' should be familiar from our work with the \verb+ggplot2+ package, which was also authored by Hadley. Functions provided by \verb+dplyr+ do in fact capture key data analysis actions (i.e., verbs). These functions include
\begin{itemize}
\item[] \texttt{mutate()} adds new variables that are functions of existing variables
\item[] \texttt{select()} picks variables based on their names
\item[] \texttt{filter()} picks cases based on their values
\item[] \texttt{summarize()} reduces multiple values down to a single summary
\item[] \texttt{arrange()} changes the ordering of the rows.
\end{itemize}
These all combine naturally with a \verb+group_by()+ function that allows you to perform any operation grouped by values of one or more variables. All the tasks done using \verb+dplyr+ can be accomplished using tools already covered in this text; however, \verb+dplyr+'s functions provide a potentially more efficient and convenient framework to accomplish these tasks. RStudio provides a convenient \href{https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf}{data wrangling cheat sheet} that covers many aspects of the \verb+tidyr+ and \verb+dplyr+ packages. 

This somewhat long section on \verb+dplyr+ adapts the nice introduction by Jenny Bryan, available at \url{http://stat545-ubc.github.io/block010_dplyr-end-single-table.html}. 

\subsection{Improved data frames}
The \verb+dplyr+ package provides two functions that offer improvements on data frames. First, \verb+data_frame()+ is a trimmed down version of the \verb+data.frame()+ function that is somewhat more user friendly, and won't be discussed here. Second, \verb+tbl_df()+ creates a tibble\footnote{Reminds me of \href{https://en.wikipedia.org/wiki/The_Trouble_with_Tribbles}{The Trouble with Tribbles}.}. A tibble has two advantages over a data frame. First, when printing, it only prints the first ten rows and the columns that fit on the page, as well as some additional information about the table's dimension, data type of variables, and non-printed columns. Second, recall that subsetting a data frame can sometimes return a vector rather than a data frame (if only one row or column is the result of the subset), a tibble does not have this behavior. Here is an example using the \verb+religionWide+ data frame\footnote{The text printed immediately after \texttt{library(dplyr)} means the \texttt{stats} and \texttt{base} packages, which are automatically loaded when you start R, have functions with the same name as functions in \texttt{dplyr}. So, for example, if you call the \texttt{filter()} or \texttt{lag()}, R will use \texttt{library(dplyr)}'s functions. Use the \texttt{::} operator to explicity identify which packages' function you want to use, e.g., if you want \texttt{stats}'s \texttt{lag()} then call \texttt{stats::lag()}.}.
<<>>=
library(dplyr)
head(religionWide)
religionWide[,1]
religionWideTbl <- tbl_df(religionWide)
head(religionWideTbl)
religionWideTbl[,1]
@
As seen above, note that once the data frame is reduced to one dimension by subsetting to one column, it is no longer a data frame and has been \emph{simplified} to a vector. This might not seem like a big deal; however, it can be very frustrating and potentially break your code when you expect an object to behave like a data frame and it doesn't because it's now a vector. Alternatively, once we convert \verb+religionWide+ to a \verb+tbl_df+ via the \verb+tbl_df()+ function the object remains a data frame even when subsetting down to one dimension (there is no automatic simplification). Converting data frames using \verb+tbl_df()+ is not required for using \verb+ddply+ but is convenient. Also, it is important to note that \verb+tbl_df+ is simply a wrapper around a data frame that provides some additional behaviors. The newly formed \verb+tbl_df+ object will still behave like a data frame (because it technically still is a data frame) but will have some added niceties (some of which are illustrated below).

\subsection{Filtering data by row}
Recall the \verb+gapminder+ data. These data are available in tab-separated format in \verb+gapminder.tsv+, and can be read in using \verb+read.delim()+ (or the related read functions described previously). The \verb+read.delim()+ function defaults to \verb+header = TRUE+ so this doesn't need to be specified explicitly. In this section we will be working with the \verb+gapminder+ data often, so will use a short name for the data frame to save typing.
<<>>=
u.gm <- "http://blue.for.msu.edu/FOR875/data/gapminder.tsv"
gm <- read.delim(u.gm)
gm <- tbl_df(gm)
str(gm)
head(gm)
@

Filtering helps us to examine subsets of the data such as data from a particular country, from several specified countries, from certain years, from countries with certain populations, etc. Some examples:

<<echo = FALSE>>=
options(scipen = 100)
@
<<>>=
filter(gm, country == "Brazil")
filter(gm, country %in% c("Brazil", "Mexico"))
filter(gm, country %in% c("Brazil", "Mexico") & year %in% c(1952, 1972))
filter(gm, pop > 300000000)
filter(gm, pop > 300000000 & year == 2007)
@
Notice the full results are not printed. For example, when we asked for the data for Brazil and Mexico, only the first ten rows were printed. This is an effect of using the \verb+tbl_df()+ function. Of course if we wanted to analyze the results (as we will below) the full set of data would be available. 

\subsection{Selecting variables by column}
Continuing with the \verb+gapminder+ data, another common task is to restrict attention to some subset of variables in the data set. The \Rfunc{select()} function does this.

<<>>=
select(gm, country, year, lifeExp)
select(gm, 2:4)
select(gm, -c(2,3,4))
select(gm, starts_with("c"))
@
Notice a few things. Variables can be selected by name or column number. As usual, a negative sign tells R to leave something out. And there are special functions such as \verb+starts_with+ that provide ways to match part of a variable's name. 

\subsection{Pipes}
Consider selecting the country, year, and population for countries in Asia or Europe. One possibility is to nest a \Rfunc{filter()} function inside a \verb+select()+ function.
<<>>=
select(filter(gm, continent %in% c("Asia", "Europe")), country, year, pop)
@
Even a two-step process like this becomes hard to follow in this nested form, and often we will want to perform more than two operations. There is a nice feature in \verb+dplyr+ that allows us to ``feed'' results of one function into the first argument of a subsequent function. Another way of saying this is that we are ``piping'' the results into another function. The \verb+%>%+ operator does the piping.
Here we again restrict attention to country, year, and population for countries in Asia or Europe. 
<<>>=
gm %>% filter(continent %in% c("Asia", "Europe")) %>% select(country, year, pop)
@ 
It can help to think of \verb+%>%+ as representing the word ``then.'' The above can be read as, ``Start with the data frame \verb+gm+ \emph{then} filter it to select data from the continents Asia and Europe \emph{then} select the variables country, year, and population from these data.''

The pipe operator \verb+%>%+ is not restricted to functions in \verb+dplyr+. In fact the pipe operator itself was introduced in another package called \verb+magrittr+, but is included in \verb+dplyr+ as a convenience.

\subsection{Arranging data by row}
By default the \verb+gapminder+ data are arranged by country and then by year. 
<<>>=
head(gm, 15)
@
Possibly arranging the data by year and then country would be desired. The \verb+arrange()+ function makes this easy. We will again use pipes.
<<>>=
gm %>% arrange(year, country)
@
How about the data for Rwanda, arranged in order of life expectancy.
<<>>=
gm %>% filter(country == "Rwanda") %>% arrange(lifeExp)
@
Possibly we want these data to be in decreasing (descending) order. Here, \verb+desc()+ is one of many \verb+dplyr+ helper functions.
<<>>=
gm %>% filter(country == "Rwanda") %>% arrange(desc(lifeExp))
@
Possibly we want to include only the year and life expectancy, to make the message more stark.
<<>>=
gm %>% filter(country == "Rwanda") %>% select(year, lifeExp) %>% arrange(desc(lifeExp)) 
@
For analyzing data in R, the order shouldn't matter. But for presentation to human eyes, the order is important.

\subsection{Renaming variables}
The \verb+dplyr+ package has a \verb+rename+ function that makes renaming variables in a data frame quite easy.
<<>>=
gm <- rename(gm, population = pop)
head(gm)
@

\subsection{Data summaries and grouping}
The \verb+summarize()+ function computes summary statistics using user provided functions for one or more columns of data in a data frame. 
<<>>=
summarize(gm, meanpop = mean(population), medpop = median(population))
##or
gm %>% summarize(meanpop = mean(population), 
                 medpop = median(population))
@
Often we want summaries for specific components of the data. For example, we might want the median life expectancy for each continent separately. One option is subsetting:
<<>>=
median(gm$lifeExp[gm$continent == "Africa"])
median(gm$lifeExp[gm$continent == "Asia"])
median(gm$lifeExp[gm$continent == "Europe"])
median(gm$lifeExp[gm$continent == "Americas"])
median(gm$lifeExp[gm$continent == "Oceania"])
@
The \verb+group_by()+ function makes this easier, and makes the output more useful.
<<>>=
gm %>% group_by(continent) %>% summarize(medLifeExp = median(lifeExp)) 
@
Or if we want the results ordered by the median life expectancy:
<<tidy=FALSE>>=
gm %>% group_by(continent) %>% 
    summarize(medLifeExp = median(lifeExp)) %>% 
    arrange(medLifeExp)
@
As another example, we calculate the number of observations we have per continent (using the \verb+n()+ helper function), and then, among continents, how many distinct countries are represented (using \verb+n_distinct()+). 
<<tidy=FALSE>>=
gm %>% group_by(continent) %>% summarize(numObs = n())
gm %>%
  group_by(continent) %>%
  summarize(n_obs = n(), n_countries = n_distinct(country))
@

Here is a bit more involved example that calculates the minimum and maximum life expectancies for countries in Africa by year. 
<<tidy=FALSE>>=
gm %>%
  filter(continent == "Africa") %>%
  group_by(year) %>%
  summarize(min_lifeExp = min(lifeExp), max_lifeExp = max(lifeExp))
@
This is interesting, but the results don't include the countries that achieved the minimum and maximum life expectancies. Here is one way to achieve that. We will start with the minimum life expectancy. Note the rank of the minimum value will be 1.
<<tidy=FALSE>>=
gm %>% select(country, continent, year, lifeExp) %>% 
    group_by(year) %>% arrange(year) %>% 
    filter(rank(lifeExp) == 1)
@
Next we add the maximum life expectancy. Here we need to better understand the \Rfunc{desc()} function, which will transform a vector into a numeric vector which will be sorted in descending order. Here are some examples.
<<>>=
desc(1:5)
desc(c(2,3,1,5,6,-4))
desc(c("a", "c", "b", "w", "e"))
@
We now use this to extract the maximum life expectancy. Recall that \verb+|+ represents ``or.'' Also by default only the first few rows of a \verb+tbl_df+ object will be printed. To see all the rows we pipe the output to \verb+print(n = 24)+ to ask for all 24 rows to be printed.
<<tidy=FALSE>>=
gm %>% select(country, continent, year, lifeExp) %>% 
    group_by(year) %>% arrange(year) %>% 
    filter(rank(lifeExp) == 1 | rank(desc(lifeExp)) == 1) %>% 
    print(n=24)
@

\subsection{Creating new variables}
The \verb+$+ notation provides a simple way to create new variables in a data frame. The \Rfunc{mutate()} function provides another, sometimes cleaner way to do this. We will use \Rfunc{mutate()} along with the \Rfunc{lag()} function to investigate changes in life expectancy over five years for the \verb+gapminder+ data. We'll do this in a few steps. First, we create a variable that measures the change in life expectancy and remove the population and GDP variables that are not of interest. We have to be careful to first group by country, since we want to calculate the change in life expectancy by country.
<<>>=
gm %>% group_by(country) %>% 
  mutate(changeLifeExp = lifeExp - lag(lifeExp, order_by = year)) %>% 
  select(-c(population, gdpPercap))
@

Next, summarize by computing the largest drop in life expectancy. 
<<>>=
gm %>% group_by(country) %>% 
  mutate(changeLifeExp = lifeExp - lag(lifeExp, order_by = year)) %>% 
  select(-c(population, gdpPercap)) %>% 
  summarize(largestDropLifeExp = min(changeLifeExp))
@
Oops. We forgot that since we don't have data from before 1952, the first drop will be \verb+NA+. Let's try again. 
<<>>=
gm %>% group_by(country) %>% 
  mutate(changeLifeExp = lifeExp - lag(lifeExp, order_by = year)) %>% 
  select(-c(population, gdpPercap)) %>% 
  summarize(largestDropLifeExp = min(changeLifeExp, na.rm = TRUE))
@
That's not quite what we wanted. We could arrange the results by the life expectancy drop, but it would be good to have both the continent and year printed out also. So we'll take a slightly different approach, by arranging the results in increasing order.
<<>>=
gm %>% group_by(country) %>% 
  mutate(changeLifeExp = lifeExp - lag(lifeExp, order_by = year)) %>% 
  select(-c(population, gdpPercap)) %>% 
  arrange(changeLifeExp)
@
That's still not quite right. Because the data are grouped by country, R did the ordering within group. If we want to see the largest drops overall, we need to remove the grouping.
<<>>=
gm %>% group_by(country) %>% 
  mutate(changeLifeExp = lifeExp - lag(lifeExp, order_by = year)) %>% 
  select(-c(population, gdpPercap)) %>% 
  ungroup() %>% 
  arrange(changeLifeExp) %>% 
  print(n=20)
@

\begin{hw}
{Learning objectives: introduce {\tt with}, {\tt tapply}, and {\tt cut} functions; summarize data using the {\tt table} function with logical subsetting; practice using factor data types.}
\end{hw}

\begin{hw}
{Learning objectives: work with messy data; import data from an external spreadsheet; practice using functions in {\tt tidyr} and {\tt ggplot2}.}
\end{hw}

\begin{hw}
{Learning objectives: work with several key {\tt dplyr} functions; manipulate data frames (actually tibbles); summarize and visualize data from large data files.}
\end{hw}
