\chapter{Working with Data}
Bringing data into R, exporting data from R in a form that is readable by other software, cleaning and reshaping data, and other data manipulation tasks are an important and often overlooked component of data science. The book \citet{SpectorDataManipulation}, while a few years old, is still an excellent reference for data-related issues. And the \emph{R Data Import/Export} manual, available online at \url{https://cran.r-project.org/doc/manuals/r-release/R-data.html}, is an up-to-date (and free) reference on importing a wide variety of datasets into R and on exporting data in various forms.

\section{Reading data into R}
Data come in a dizzying variety of forms. It might be in a proprietary format such as an \url{.xlsx} Excel file, a \url{.sav} SPSS file, or a \url{.mtw} Minitab file. It might be part of a web site such as Wikipedia. It might be part of a database. And so on. Fortunately many datasets are (or can be) saved as plain text files, and most software can both read and write such files, so our initial focus will be on reading plain text files into R and saving data from R in plain text format.

(The \verb+foreign+ R package provides functions to directly read data saved in some of the proprietary formats into R, which is sometimes unavoidable, but if possible it is good to save data from  another package as plain text and then read this plain text file into R. In Chapter~\ref{CHAPTER:XML} methods for reading web-based data sets into R will be discussed.)

The function \verb+read.table()+ and its offshoots such as \verb+read.csv()+ are used to read in rectangular data from a text file. For example, the file \url{BrainAndBody.csv} contains data\footnote{These data come from the \texttt{MASS} R library.} on the brain weight, body weight, and name of some terrestrial animals. Here are the first few lines of that file:
\begin{verbatim}
body,brain,name
1.35,8.1,Mountain beaver
465,423,Cow
36.33,119.5,Grey wolf
27.66,115,Goat
1.04,5.5,Guinea pig
\end{verbatim}
As is evident, the first line of the file contains the names of the three variables, separated (delimited) by commas. Each subsequent line contains the body weight, brain weight, and name of a specific terrestrial animal.

This file is accessible at the url \url{http://blue.for.msu.edu/STT301/data/BrainAndBody.csv}. The \verb+read.table()+ function is used to read these data into an R data frame.
<<>>=
u.bb = "http://blue.for.msu.edu/STT301/data/BrainAndBody.csv"
BrainBody = read.table(file = u.bb, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(BrainBody)
@
The arguments used in this call to \verb+read.table()+ include:
\begin{enumerate}
\item \verb+file = u.bb+, which tells R the location of the file. In this case the string \url{http://blue.for.msu.edu/STT301/data/BrainAndBody.csv} giving the location is rather long, so it was first assigned to the object \verb+u.bb+.
\item \verb+header = TRUE+, which tells R that the first line of the file gives the names of the variables.
\item \verb+sep = ","+, which tells R that a comma separates the fields in the file.
\item \verb+stringsAsFactors = FALSE+ which tells R not to convert character vectors to factors.
\end{enumerate}
The function \verb+read.csv()+ is the same as \verb+read.table()+ except the default separator is a comma, whereas the default separator for \verb+read.table()+ is whitespace. 

The file \url{BrainAndBody.tsv} contains the same data, except a tab is used in place of a comma to separate fields. The only change needed to read in the data in this file is in the \verb+sep+ argument (and of course the \verb+file+ argument, since the data are stored in a different file):
<<>>=
u.bb = "http://blue.for.msu.edu/STT301/data/BrainAndBody.tsv"
BrainBody2 = read.table(file = u.bb, header = TRUE, sep = "\t", stringsAsFactors = FALSE)
head(BrainBody2)
@

A third file, \url{BrainAndBody.txt}, contains the same data, but also contains a few lines of explanatory text above the names of the variables. It also uses whitespace rather than a comma or a tab as a separator. Here are the first few lines of the file.
\begin{verbatim}
This file contains data
on brain and body
weights of several terrestrial animals

"body" "brain" "name"
1.35 8.1 "Mountain beaver"
465 423 "Cow"
36.33 119.5 "Grey wolf"
27.66 115 "Goat"
1.04 5.5 "Guinea pig"
11700 50 "Dipliodocus"
2547 4603 "Asian elephant"
\end{verbatim}
Notice that in this file the values of \verb+name+ are put inside of quotation marks. This is necessary since instead R would (reasonably) assume that the first line contained the values of four variables, the values being \verb+1.35+, \verb+8.1+, \verb+Mountain+, and \verb+beaver+ while in reality there are only three values desired, with \verb+Mountain beaver+ being the third. 

To read in this file we need to tell R to skip the first five lines, and also that whitespace is the separator. The \verb+skip+ argument handles the first, and the \verb+sep+ argument the second. First let's see what happens if we don't use the \verb+skip+ argument.
<<>>=
u.bb = "http://blue.for.msu.edu/STT301/data/BrainAndBody.txt"
BrainBody3 = read.table(u.bb, header = TRUE, sep = " ", stringsAsFactors = FALSE)
@
R assumed that the first line of the file contained the variable names, since \verb+header = TRUE+ was specified, and counted four including \verb+This+, \verb+file+, \verb+contains+, and \verb+data+. So in the first line of actual data, R expected four columns containing data plus possibly a fifth column containing row names for the data set, and complained that ``line 1 did not have 5 elements." The error message is somewhat mysterious, since it starts with ``Error in scan.'' This happens because \verb+read.table()+ actually uses a more basic R function called \verb+scan()+ to do the work. 

Here's how to read in the file correctly.
<<>>=
u.bb = "http://blue.for.msu.edu/STT301/data/BrainAndBody.txt"
BrainBody3 = read.table(u.bb, header = TRUE, sep = " ", stringsAsFactors = FALSE, skip = 4)
BrainBody3[1:10,]
@

\subsection{Reading data with missing observations}
Missing data is represented in many ways. Sometimes missing data is just that, i.e., the place where it should be in the file is blank. Other times specific numbers such as $-999$ or specific symbols are used. The \verb+read.table()+ function has an argument \verb+na.string+ which allows the user to specify how missing data is indicated in the source file.

The site \url{http://www.wunderground.com/history/} makes weather data available for locations around the world, and for dates going back to 1945. The file \url{WeatherKLAN2014.csv} contains weather data for Lansing, Michigan for the year 2014. Here are the first few lines of that file:
\begin{verbatim}
EST,Max TemperatureF,Min TemperatureF, Max VisibilityMiles, Min VisibilityMiles,PrecipitationIn, CloudCover, Events
1/1/14,14,9,10,1,0.08,8,Snow
1/2/14,13,-3,9,0,0.01,7,Snow
1/3/14,13,-11,10,5,0,1,Snow
1/4/14,31,13,10,1,0.12,5,Snow
1/5/14,29,16,4,0,0.78,8,Fog-Snow
1/6/14,16,-12,10,0,0.07,8,Fog-Snow
1/7/14,2,-13,10,1,T,6,Snow
1/8/14,17,-1,10,8,T,5,Snow
1/9/14,21,2,10,2,0.01,7,Snow
1/10/14,39,21,9,0,0.39,8,Fog-Rain-Snow
1/11/14,41,32,10,0,0.16,8,Fog-Rain
1/12/14,39,31,10,9,0,4,
\end{verbatim}
Look at the last line, and notice that instead of an \verb+Event+ such as \verb+Snow+ or \verb+Fog-Snow+ there is nothing after the comma. This observation is missing, but rather than using an explicit code such as \verb+NA+, the site just leaves that entry blank. To read these data into R we will supply the argument \verb+na.string = ""+ which tells R that the file indicates missing data by leaving the appropriate entry blank.
<<>>=
u.weather = "http://blue.for.msu.edu/STT301/data/WeatherKLAN2014.csv"
WeatherKLAN2014 = read.csv(u.weather, header=TRUE, stringsAsFactors = FALSE, na.string = "")
WeatherKLAN2014[1:15,]
@

\section{Summarizing data frames}
Some common data tasks include variable summaries such as means or standard deviations, transforming an existing variable, and creating new variables. As with many tasks, there are several ways to accomplish each of these.

\subsection{Column (and row) summaries}
The file \url{WeatherKLAN2014Full.csv} contains a more complete set of weather data variables than \url{WeatherKLAN2014.csv}, from the same source, \url{http://www.wunderground.com/history}. 
<<>>=
u.weather = "http://blue.for.msu.edu/STT301/data/WeatherKLAN2014Full.csv"
WeatherKLAN2014Full = read.csv(u.weather, header=TRUE, stringsAsFactors = FALSE, na.string = "")
names(WeatherKLAN2014Full)
@
How can we compute the mean for each variable? One possibility is to do this a variable at a time:
<<>>=
mean(WeatherKLAN2014Full$Mean.TemperatureF)
mean(WeatherKLAN2014Full$Min.TemperatureF)
mean(WeatherKLAN2014Full$Max.TemperatureF)
##Et Cetera
@
This is pretty inefficient. Fortunately there is a \verb+colMeans()+ function which computes the mean of each (or specified) columns in a data frame. Some columns in the current data frame are not numeric, and obviously we don't want to ask R to compute means for these columns. We use \verb+str()+ to investigate.
<<>>=
str(WeatherKLAN2014Full)
@
It isn't surprising that \verb+EST+ and \verb+Events+ are not numeric, but is surprising that \verb+PrecipitationIn+, which measures precipitation in inches, also is not numeric, but is character. Let's investigate further.
<<>>=
WeatherKLAN2014Full$PrecipitationIn[1:50]
@
Now it's more clear. The original data file included \verb+T+ in the precipitation column to represent a ``trace'' of precipitation, which is precipitation greater than $0$ but less than $0.01$ inches. One possibility would be to set all these values to \verb+"0"+ , and then to convert the column to numeric. For now we will just leave the \verb+PrecipitationIn+ column out of the columns for which we request the mean.
<<>>=
colMeans(WeatherKLAN2014Full[,c(2:19, 21, 23)])
@

\subsection{The \texttt{apply()} function}
R also has functions \verb+rowMeans()+, \verb+colSums()+, and \verb+rowSums()+. But what if we want to compute the median or standard deviation of columns of data, or some other summary statistic? For this the \verb+apply()+ function can be used. This function applies a user-chosen function to either the rows or columns (or both) of a data frame. The arguments are \verb+X+, the data frame of interest, \verb+MARGIN+, specifying either rows (\verb+MARGIN = 1+) or columns (\verb+MARGIN = 2+), and \verb+FUN+, the function to be applied.
<<>>=
apply(X = WeatherKLAN2014Full[,c(2:19, 21, 23)], MARGIN = 2, FUN = sd)
@
As with any R function the arguments don't need to be named, so 
\begin{verbatim}
apply(WeatherKLAN2014Full[,c(2:19, 21, 23)], 2, sd)
\end{verbatim}
has the same result. 

\subsection{Saving typing using \texttt{with()}}
Consider calculating the mean of the maximum temperature values for those days where the cloud cover is less than 4 and when the maximum humidity is over 85. We can do this using subsetting.
<<>>=
mean(WeatherKLAN2014Full$Max.TemperatureF[WeatherKLAN2014Full$CloudCover < 4 & WeatherKLAN2014Full$Max.Humidity > 85])
@
While this works, it requires a lot of typing, since each time we refer to a variable in the data set we need to preface its name by \verb+WeatherKLAN2014Full$+. The \verb+with()+ function tells R that we are working with a particular data frame, and we don't need to keep typing the name of the data frame.
<<>>=
with(WeatherKLAN2014Full, mean(Max.TemperatureF[CloudCover < 4 & Max.Humidity > 85]))
@

\section{Transforming a data frame}
Commonly variables are added to, removed from, changed in, or rearranged in, a data frame. The subsetting features of R make this reasonably easy. We will investigate this in the context of the \verb+gapminder+ data frame. If the \verb+gapminder+ library is not  yet installed, use \verb+install.packages("gapminder")+ to install it locally.

<<>>=
library(gapminder)
str(gapminder)
@

\subsection{Adding variables}
The data frame contains per capita GDP and population, and it might be interesting to create a variable which gives the total GDP by multiplying these two variables. (If we were interested in an accurate value for the total GDP we would probably be better off getting this information directly, since it is likely that the per capita GDP values in the data frame are rounded substantially.) 

<<>>=
gapminder$TotalGDP = gapminder$gdpPercap * gapminder$pop
str(gapminder)
@

Analogous to the \verb+with()+ function, there is a function \verb+within()+ which can simplify the syntax. Whereas \verb+with()+ does not change the data frame, \verb+within()+ can.

<<>>=
rm(gapminder)
library(gapminder)
str(gapminder)
gapminder <- within(gapminder, TotalGDP <- gdpPercap * pop)
str(gapminder)
@

A nice feature of \verb+within()+ is its ability to add more than one variable at a time to a data frame. In this case the two or more formulas creating new variables must be enclosed in braces.
<<>>=
gapminder <- within(gapminder, {TotalGDP <- gdpPercap * pop
                                                     lifeExpMonths = lifeExp * 12})
str(gapminder)
@

\subsection{Removing variables}
After reflection we may realize that the new variables we added to the \verb+gapminder+ data frame are not useful, and should be removed.
<<>>=
str(gapminder)
gapminder = gapminder[1:6]
str(gapminder)
@
The same result could be obtained via \verb+gapminder = gapminder[, 1:6]+. The first method uses the fact that a data frame is also a list, and uses list subsetting methods. It is slightly preferable, since even if only one variable is retained, the object will still be a data frame, while the other method can return a vector in this case.
<<>>=
a = data.frame(x = 1:3, y = c("dog", "cat", "pig"), z = seq(from = 1, to = 2, length = 3))
a
a = a[1]
a 
a = data.frame(x = 1:3, y = c("dog", "cat", "pig"), z = seq(from = 1, to = 2, length = 3))
a 
a = a[,1]
a
@

\subsection{Transforming variables}
Consider the gapminder data again. Possibly we don't want to add a new variable which gives life expectancy in months, but rather want to modify the existing variable to measure life expectancy in years. Here are two ways to accomplish this.
<<>>=
rm(gapminder)
library(gapminder)
gapminder$lifeExp[1:5]
gapminder$lifeExp = gapminder$lifeExp * 12
gapminder$lifeExp[1:5]
rm(gapminder)
library(gapminder)
gapminder$lifeExp[1:5]
gapminder <- within(gapminder, lifeExp <- lifeExp * 12)
gapminder$lifeExp[1:5]
@

\subsection{Rearranging variables}
Consider the full weather data set again. 
<<>>=
u.weather = "http://www.stt.msu.edu/~melfi/STT301/data/WeatherKLAN2014Full.csv"
WeatherKLAN2014Full = read.csv(u.weather, header=TRUE, stringsAsFactors = FALSE, na.string = "")
names(WeatherKLAN2014Full)
@
If we want the wind speed variables to come right after the date, we can again use subsetting.
<<>>=
WeatherKLAN2014Full = WeatherKLAN2014Full[c(1,17, 18, 19, 2:16, 20:23)]
names(WeatherKLAN2014Full)
@

\section{Reshaping data}
A data set can be represented in several different formats. Consider a (fictitious) data set on incomes of three people during three different years. Here is one representation of the data:
<<echo = F>>=
yearlyIncomeWide = data.frame(name = c("John Smith", "Jane Doe", "Albert Jones"), income1990 = c(29784, 56789, 2341), income2000 = c(39210, 89321, 34567), income2010 = c(41213, 109321, 56781))
library(tidyr)
yearlyIncomeLong = gather(yearlyIncomeWide, year, income, income1990:income2010)
@
<<>>=
yearlyIncomeWide
@
Here is another representation of the same data:
<<>>=
yearlyIncomeLong
@
For hopefully obvious reasons, the first representation is called a \emph{wide} representation of the data, and the second is called a \emph{long} representation. Each has its merits. The first representation is probably easier for people to read, while the second is often the form needed for analysis by statistical software such as R. There are of course other representations. For example the rows and columns could be interchanged to create a different wide representation, or the long representation, which currently groups data by year, could group by name instead. 

Whatever the relative merits of wide and long representations of data, transforming data from wide to long or long to wide is often required. As with many tasks, there are several ways to accomplish this in R. We will focus on a relatively new library called \verb+tidyr+ which performs the transformations and more.

\subsection{Tidyr}
The R library \verb+tidyr+ has functions for converting data between formats. To illustrate its use we examine a simple data set that explores the relationship between religion and income in the United States. The data come from a Pew survey, and are used in the \verb+tidyr+ documentation to illustrate transforming data from wide to long format. 

<<>>=
u.rel = "http://www.stt.msu.edu/~melfi/STT301/data/religion2.csv"
religion = read.csv(u.rel, header=TRUE, stringsAsFactors = FALSE)
head(religion)
@

As given, the columns include religion and income level, and there are counts for each of the combinations of religion and income level. For example there are 27 people who are Agnostic and whose income is less than 10 thousand dollars, and there are 617 people who are Catholic and whose income is between 10 and 20 thousand dollars.

The \verb+gather()+ function can transform data from wide to long format. 

<<>>=
library(tidyr)
religionLong <- gather(data = religion, key = IncomeLevel, value = Frequency, 2:11)
head(religionLong)
tail(religionLong)
@
To use \verb+gather()+ we specified the data frame (\verb+data = religion+), the name we want to give to the column created from the income levels (\verb+key = IncomeLevel+), the name we want to give to the column containing the frequency values (\verb+value = Frequency+) and the columns to gather (\verb+2:11+). 

Columns to be gathered can be specified by name also, and we can also specify which columns should be omitted using a negative sign in front of the name(s). So the following creates an equivalent data frame:

<<>>=
religionLong <- gather(data = religion, key = IncomeLevel, value = Frequency, -religion)
head(religionLong)
@

<<>>=
religionWide <- spread(religionLong, IncomeLevel, Frequency)
head(religionWide)
@
Here we specify the data frame (\verb+religionLong+), the column (\verb+IncomeLevel+) to be spread, and the column of values (\verb+Frequency+) to be spread among the newly created columns. As can be seen, this particular call to \verb+spread()+ yields the original data frame.

\section{Manipulating data with \texttt{dplyr}}
Much of the effort (a figure of 80\% is sometimes suggested) in a data analysis is spent in cleaning the data and getting it ready for analysis. Having effective tools for this task can save substantial time and effort. The R package \verb+dplyr+ is designed, in the designer's words, to be, ``A fast, consistent tool for working with data frame like objects.'' This somewhat long section on \verb+dplyr+ adapts the nice introduction by Jenny Bryan, available at \url{http://stat545-ubc.github.io/block009_dplyr-intro.html} and \url{http://stat545-ubc.github.io/block010_dplyr-end-single-table.html}. 

\subsection{Improved data frames}
The \verb+dplyr+ package provides two functions which provide improvements on data frames in R. The first, \verb+data_frame()+, is a trimmed down version of the \verb+data.frame()+ function which is somewhat more user friendly, and won't be discussed here. The second, \verb+tbl_df()+, has two important features. First, when printing, it only prints the first ten rows and the columns which fit on the page, which can be quite helpful with large data frames with many variables. Also, recall that subsetting a data frame can sometimes return a vector rather than a data frame, and \verb+tbl_df()+ doesn't allow this. Here is an example using the \verb+religionWide+ data frame.
<<>>=
library(dplyr)
head(religionWide)
religionWide[,1]
religionWideTbl = tbl_df(religionWide)
head(religionWideTbl)
religionWideTbl[,1]
@
Converting data frames using \verb+tbl_df()+ is not required for using \verb+ddplyr+ but is convenient.

\subsection{Filtering data by row}
Recall the \verb+gapminder+ data. These data are available in tab-delimited format in \url{gapminder.tsv}, and can be read in using \verb+read.delim()+. The \verb+read.delim()+ function defaults to \verb+header = TRUE+ so this doesn't need to be specified explicitly. In this section we will be working with the \verb+gapminder+ data often, so will use a short name for the data frame.
<<>>=
u.gm = "http://www.stt.msu.edu/~melfi/STT301/data/gapminder.tsv"
gm = read.delim(u.gm)
gm = tbl_df(gm)
str(gapminder)
head(gm)
@

Filtering helps us to examine subsets of the data such as data from a particular country or from several specified countries, data from certain years, from countries with certain populations, etc. Some examples:

<<echo = FALSE>>=
options(scipen = 100)
@
<<>>=
filter(gm, country == "Brazil")
filter(gm, country %in% c("Brazil", "Mexico"))
filter(gm, country %in% c("Brazil", "Mexico") & year %in% c(1952, 1972))
filter(gm, pop > 300000000)
filter(gm, pop > 300000000 & year == 2007)
@
Notice that the full results are not printed. For example when we asked for the data for Brazil and Mexico, only the first ten rows were printed. This is an effect of using the \verb+tbl+df()+ function. Of course if we wanted to analyze the results (as we will below) the full set of data would be available. 

\subsection{Selecting variables by column}
Continuing with the gapminder data, another common task is to restrict attention to some subset of variables in the data set. The \Rfunc{select()} function does this.

<<>>=
select(gm, country, year, lifeExp)
select(gm, 2:4)
select(gm, -c(2,3,4))
select(gm, starts_with("c"))
@
Notice a few things. Variables can be selected by name. Variables can be selected by number. As usual, a negative sign tells R to leave something out. And there are special functions such as \verb+starts_with+ which provide ways to match part of a variable's name. 

\subsection{Pipes}
Consider selecting the country, year, and population for countries in Asia or Europe. One possibility is to nest a \Rfunc{filter()} function inside a \Rfunc{select()} function.
<<>>=
select(filter(gm, continent %in% c("Asia", "Europe")), country, year, pop)
@
Even a two-step process like this becomes hard to follow in this nested form, and often we will want to perform more than two operations. There is a nice feature in \Rpackage{dplyr} which allows us to ``feed'' the results of one function into another in a more transparent way. Another way of saying this is that we are ``piping'' the results into another function. The \verb+%>%+ operator does the piping.
Here we again restrict attention to country, year, and population for countries in Asia or Europe. 
<<>>=
gm %>% filter(continent %in% c("Asia", "Europe")) %>% select(country, year, pop)
@ 
It can help to think of \verb+%>%+ as representing the word ``then.'' The above can be read as, ``Start with the data frame \verb+gm+ \emph{then} filter it to select data from the continents Asia and Europe \emph{then} select the variables country, year, and population from these data.''

The pipe operator %>% is not restricted to functions in \Rpackage{dplyr}. In fact the pipe operator itself was introduced in another package, and is included in \Rpackage{dplylr} as a convenience.

\subsection{Arranging data by row}
By default the gapminder data are arranged by country and then by year. 
<<>>=
head(gm, 15)
@
Possibly arranging the data by year and then country would be desired. The \Rfunc{arrange()} function makes this easy. We will again use pipes.
<<>>=
gm %>% arrange(year, country)
@
How about the data for Rwanda, arranged in order of life expectancy.
<<>>=
gm %>% filter(country == "Rwanda") %>% arrange(lifeExp)
@
Possibly we want these data to be in decreasing (descending) order.
<<>>=
gm %>% filter(country == "Rwanda") %>% arrange(desc(lifeExp))
@
Possibly we want to include only the year and life expectancy, to make the message more stark.
<<>>=
gm %>% filter(country == "Rwanda") %>% select(year, lifeExp) %>% arrange(desc(lifeExp)) 
@
For analyzing data in R, the order shouldn't matter. But for presentation to human eyes, the order is important.

\subsection{Renaming variables}
The \Rpackage{dplyr} package has a \Rfunc{rename} function which makes renaming variables in a data frame quite easy.
<<>>=
gm <- rename(gm, population = pop)
head(gm)
@

\subsection{Data summaries and grouping}
The \Rfunc{summarize()} function computes summary statistics for one or more columns of data in a data frame. 
<<>>=
summarize(gm, meanpop = mean(population), medpop = median(population))
##or
gm %>% summarize(meanpop = mean(population), medpop = median(population))
@
Often we want summaries for specific components of the data. For example we might want the median life expectancy for each continent separately. One option is subsetting:
<<>>=
median(gm$lifeExp[gm$continent == "Africa"])
median(gm$lifeExp[gm$continent == "Asia"])
median(gm$lifeExp[gm$continent == "Europe"])
median(gm$lifeExp[gm$continent == "Americas"])
median(gm$lifeExp[gm$continent == "Oceania"])
@
The \verb+group_by()+ function makes this easier, and makes the output more useful.
<<>>=
gm %>% group_by(continent) %>% summarize(medLifeExp = median(lifeExp)) 
@
Or if we want the results ordered by the median life expectancy:
<<>>=
gm %>% group_by(continent) %>% summarize(medLifeExp = median(lifeExp)) %>% arrange(medLifeExp)
@
As another example, we calculate the number of observations we have per continent, and then, among continents, how many distinct countries are represented.
<<>>=
gm %>% group_by(continent) %>% summarize(numObs = n())
gm %>%
  group_by(continent) %>%
  summarize(n_obs = n(), n_countries = n_distinct(country))
@

Here is a bit more substantial example, which calculates the minimum and maximum life expectancies for countries in Africa. 
<<>>=
gm %>%
  filter(continent == "Africa") %>%
  group_by(year) %>%
  summarize(min_lifeExp = min(lifeExp), max_lifeExp = max(lifeExp))
@
This is interesting, but the results don't include the countries which achieved the minimum and maximum life expectancies. Here is one way to achieve that. We will start with the minimum life expectancy. Note that the rank of the minimum value will be 1.
<<>>=
gm %>% select(country, continent, year, lifeExp) %>% group_by(year) %>% arrange(year) %>% filter(rank(lifeExp) == 1)
@
Next we add the maximum life expectancy. Here we need to better understand the \Rfunc{desc()} function, which will transform a vector into a numeric vector which will be sorted in descending order. Here are some examples.
<<>>=
desc(1:5)
desc(c(2,3,1,5,6,-4))
desc(c("a", "c", "b", "w", "e"))
@
We now use this to extract the maximum life expectancy. Recall that \verb+|+ represents ``or.'' Also by default only the first few rows of a \verb+tbl_df+ object will be printed. To see all the rows we pipe the output to \verb+print(n = 24)+ to ask for all 24 rows to be printed.
<<>>=
gm %>% select(country, continent, year, lifeExp) %>% group_by(year) %>% arrange(year) %>% filter(rank(lifeExp) == 1 | rank(desc(lifeExp)) == 1)  %>% print(n=24)
@

\subsection{Creating new variables}
The \verb+$+ notation provides a simple way to create new variables in a data frame. The \Rfunc{mutate()} function provides another, sometimes cleaner way to do this. We will use \Rfunc{mutate()} along with the \Rfunc{lag()} function to investigate changes in life expectancy over five years for the \verb+gapminder+ data. We'll do this in a few steps. First, we create a variable which measures the change in life expectancy, and remove the population and gdp variables, which are not of interest. We have to be careful to first group by country, since we want to calculate the change in life expectancy by country.
<<>>=
gm %>% group_by(country) %>% mutate(changeLifeExp = lifeExp - lag(lifeExp, order_by = year)) %>% select(-c(population, gdpPercap))
@

Next, summarize by computing the largest drop in life expectancy. 
<<>>=
gm %>% group_by(country) %>% mutate(changeLifeExp = lifeExp - lag(lifeExp, order_by = year)) %>% select(-c(population, gdpPercap)) %>% summarize(largestDropLifeExp = min(changeLifeExp))
@
Oops. We forgot that since we don't have data from before 1952, the first drop will be \verb+NA+. Let's try again. 
<<>>=
gm %>% group_by(country) %>% mutate(changeLifeExp = lifeExp - lag(lifeExp, order_by = year)) %>% select(-c(population, gdpPercap)) %>% summarize(largestDropLifeExp = min(changeLifeExp, na.rm = TRUE))
@
That's not quite what we wanted. We could arrange the results by the life expectancy drop, but it would be good to have both the continent and year printed out also. So we'll take a slightly different approach, by arranging the results in increasing order.
<<>>=
gm %>% group_by(country) %>% mutate(changeLifeExp = lifeExp - lag(lifeExp, order_by = year)) %>% select(-c(population, gdpPercap)) %>% arrange(changeLifeExp)
@
That's still not quite right. Because the data are grouped by country, R did the ordering within group. If we want to see the largest drops overall, we need to remove the grouping.
<<>>=
gm %>% group_by(country) %>% mutate(changeLifeExp = lifeExp - lag(lifeExp, order_by = year)) %>% select(-c(population, gdpPercap)) %>% ungroup() %>% arrange(changeLifeExp) %>% print(n=20)
@

