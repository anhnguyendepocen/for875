library(knitr)
setwd("~/Documents/01-summer2018/for875-18/10-classification/ex-class")
purl("exercise+classification_solution.Rmd")
purl("exercise_classification_solution.Rmd")
## ----global_options------------------------------------------------------
knitr::opts_chunk$set(comment = NA, tidy = TRUE)
## ------------------------------------------------------------------------
library(ISLR)
str(Weekly)
## ---- message=FALSE, tidy = FALSE----------------------------------------
library(ggplot2)
library(dplyr)
Weekly <- mutate(Weekly, binDirection=ifelse(Direction == "Up", 1, 0))
ggplot(Weekly, aes(x = Lag1, y = binDirection)) +
geom_point() +
stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
## ------------------------------------------------------------------------
##TODO 13.1: Draw the five remaining scatter plots here, i.e., Lag2, Lag3, Lag4, Lag5, Volume, versus binDirection
ggplot(Weekly, aes(x = Lag2, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Lag3, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Lag4, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Lag5, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Volume, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
## ------------------------------------------------------------------------
Weekly.train <- subset(Weekly, Year < 2009)
Weekly.test <- subset(Weekly, Year >= 2009)
## ------------------------------------------------------------------------
lr.all <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly.train, family = "binomial" )
summary(lr.all)
## ------------------------------------------------------------------------
##TODO 13.4: Obtain the predictions and display the confusion matrix and the percent of correct predictions here.
percent.correct <- function(x){100*sum(diag(x))/sum(x)}
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.1 <- table(pred.1, Weekly.test$Direction)
x.1
percent.correct(x.1)
## ------------------------------------------------------------------------
##TODO 13.6: Fit a logistic regression model with `Lag1` as the only predictor, and display the confusion matrix and the percent of correct predictions.
lr.all <- glm(Direction ~ Lag1, data = Weekly.train, family = "binomial" )
summary(lr.all)
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.2 <- table(pred.1, Weekly.test$Direction)
x.2
percent.correct(x.2)
## ------------------------------------------------------------------------
##TODO 13.7: Fit a logistic regression model with `Lag2` as the only predictor, and display the confusion matrix and the percent of correct predictions.
lr.all <- glm(Direction ~ Lag2, data = Weekly.train, family = "binomial" )
summary(lr.all)
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.3 <- table(pred.1, Weekly.test$Direction)
x.3
percent.correct(x.3)
## ------------------------------------------------------------------------
##TODO 13.8: Fit a logistic regression model with `Volume` as the only predictor, and display the confusion matrix and the percent of correct predictions.
lr.all <- glm(Direction ~ Volume, data = Weekly.train, family = "binomial" )
summary(lr.all)
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.4 <- table(pred.1, Weekly.test$Direction)
x.4
percent.correct(x.4)
## ------------------------------------------------------------------------
##TODO 13.10: Fit a k nearest neighbors model with `Lag2` as the only predictor and k=1, and display the confusion matrix and the percent of correct predictions.
library(class)
set.seed(875)
blah <- knn(Weekly.train[3], Weekly.test[3], Weekly.train[,9], k = 1, prob = TRUE)
k.1 <- table(blah, Weekly.test[,9])
k.1
percent.correct(k.1)
## ------------------------------------------------------------------------
##TODO 13.11: Fit a k nearest neighbors model with `Lag2` as the only predictor and k=3, and display the confusion matrix and the percent of correct predictions.
set.seed(875)
blah <- knn(Weekly.train[3], Weekly.test[3], Weekly.train[,9], k = 3, prob = TRUE)
k.3 <- table(blah, Weekly.test[,9])
k.3
percent.correct(k.3)
## ------------------------------------------------------------------------
##TODO 13.12: Fit a k nearest neighbors model with `Lag2` as the only predictor and k=5, and display the confusion matrix and the percent of correct predictions.
set.seed(875)
blah <- knn(Weekly.train[3], Weekly.test[3], Weekly.train[,9], k = 5, prob = TRUE)
k.5 <- table(blah, Weekly.test[,9])
k.5
percent.correct(k.5)
## ----global_options------------------------------------------------------
knitr::opts_chunk$set(comment = NA, tidy = TRUE)
## ------------------------------------------------------------------------
library(ISLR)
str(Weekly)
## ---- message=FALSE, tidy = FALSE----------------------------------------
library(ggplot2)
library(dplyr)
Weekly <- mutate(Weekly, binDirection=ifelse(Direction == "Up", 1, 0))
ggplot(Weekly, aes(x = Lag1, y = binDirection)) +
geom_point() +
stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
## ------------------------------------------------------------------------
##TODO 13.1: Draw the five remaining scatter plots here, i.e., Lag2, Lag3, Lag4, Lag5, Volume, versus binDirection
ggplot(Weekly, aes(x = Lag2, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Lag3, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Lag4, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Lag5, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Volume, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
## ------------------------------------------------------------------------
Weekly.train <- subset(Weekly, Year < 2009)
Weekly.test <- subset(Weekly, Year >= 2009)
## ------------------------------------------------------------------------
lr.all <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly.train, family = "binomial" )
summary(lr.all)
## ------------------------------------------------------------------------
##TODO 13.4: Obtain the predictions and display the confusion matrix and the percent of correct predictions here.
percent.correct <- function(x){100*sum(diag(x))/sum(x)}
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.1 <- table(pred.1, Weekly.test$Direction)
x.1
percent.correct(x.1)
## ------------------------------------------------------------------------
##TODO 13.6: Fit a logistic regression model with `Lag1` as the only predictor, and display the confusion matrix and the percent of correct predictions.
lr.all <- glm(Direction ~ Lag1, data = Weekly.train, family = "binomial" )
summary(lr.all)
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.2 <- table(pred.1, Weekly.test$Direction)
x.2
percent.correct(x.2)
## ------------------------------------------------------------------------
##TODO 13.7: Fit a logistic regression model with `Lag2` as the only predictor, and display the confusion matrix and the percent of correct predictions.
lr.all <- glm(Direction ~ Lag2, data = Weekly.train, family = "binomial" )
summary(lr.all)
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.3 <- table(pred.1, Weekly.test$Direction)
x.3
percent.correct(x.3)
rm(list = ls())
## ----global_options------------------------------------------------------
knitr::opts_chunk$set(comment = NA, tidy = TRUE)
## ------------------------------------------------------------------------
library(ISLR)
str(Weekly)
## ---- message=FALSE, tidy = FALSE----------------------------------------
library(ggplot2)
library(dplyr)
Weekly <- mutate(Weekly, binDirection=ifelse(Direction == "Up", 1, 0))
ggplot(Weekly, aes(x = Lag1, y = binDirection)) +
geom_point() +
stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
## ------------------------------------------------------------------------
##TODO 13.1: Draw the five remaining scatter plots here, i.e., Lag2, Lag3, Lag4, Lag5, Volume, versus binDirection
ggplot(Weekly, aes(x = Lag2, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Lag3, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Lag4, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Lag5, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Volume, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
## ------------------------------------------------------------------------
Weekly.train <- subset(Weekly, Year < 2009)
Weekly.test <- subset(Weekly, Year >= 2009)
## ------------------------------------------------------------------------
lr.all <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly.train, family = "binomial" )
summary(lr.all)
## ------------------------------------------------------------------------
##TODO 13.4: Obtain the predictions and display the confusion matrix and the percent of correct predictions here.
percent.correct <- function(x){100*sum(diag(x))/sum(x)}
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.1 <- table(pred.1, Weekly.test$Direction)
x.1
percent.correct(x.1)
## ------------------------------------------------------------------------
##TODO 13.6: Fit a logistic regression model with `Lag1` as the only predictor, and display the confusion matrix and the percent of correct predictions.
lr.all <- glm(Direction ~ Lag1, data = Weekly.train, family = "binomial" )
summary(lr.all)
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.2 <- table(pred.1, Weekly.test$Direction)
x.2
percent.correct(x.2)
## ------------------------------------------------------------------------
##TODO 13.7: Fit a logistic regression model with `Lag2` as the only predictor, and display the confusion matrix and the percent of correct predictions.
lr.all <- glm(Direction ~ Lag2, data = Weekly.train, family = "binomial" )
summary(lr.all)
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.3 <- table(pred.1, Weekly.test$Direction)
x.3
percent.correct(x.3)
lr.all <- glm(Direction ~ Volume, data = Weekly.train, family = "binomial" )
summary(lr.all)
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.4 <- table(pred.1, Weekly.test$Direction)
x.4
percent.correct(x.4)
knn1 <- knn(Weekly.train[,3], Weekly.test[,3], Weekly.train[,8], k = 1, prob = TRUE)
## ------------------------------------------------------------------------
##TODO 13.10: Fit a k nearest neighbors model with `Lag2` as the only predictor and k=1, and display the confusion matrix and the percent of correct predictions.
library(class)
set.seed(875)
blah <- knn(Weekly.train[3], Weekly.test[3], Weekly.train[,8], k = 1, prob = TRUE)
k.1 <- table(blah, Weekly.test[,9])
k.1
percent.correct(k.1)
knn1 <- knn(Weekly.train[,3], Weekly.test[,3], Weekly.train[,8], k = 1, prob = TRUE)
str(Weekly.train)
rm(list = ls())
## ------------------------------------------------------------------------
library(ISLR)
str(Weekly)
## ---- message=FALSE, tidy = FALSE----------------------------------------
library(ggplot2)
library(dplyr)
Weekly <- mutate(Weekly, binDirection=ifelse(Direction == "Up", 1, 0))
ggplot(Weekly, aes(x = Lag1, y = binDirection)) +
geom_point() +
stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
## ------------------------------------------------------------------------
##TODO 13.1: Draw the five remaining scatter plots here, i.e., Lag2, Lag3, Lag4, Lag5, Volume, versus binDirection
ggplot(Weekly, aes(x = Lag2, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Lag3, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Lag4, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Lag5, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
ggplot(Weekly, aes(x = Volume, y = binDirection)) + geom_point() + stat_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE)
## ------------------------------------------------------------------------
Weekly.train <- subset(Weekly, Year < 2009)
Weekly.test <- subset(Weekly, Year >= 2009)
str(Weekly.train)
str(Weekly.test)
## ------------------------------------------------------------------------
lr.all <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly.train, family = "binomial" )
summary(lr.all)
## ------------------------------------------------------------------------
##TODO 13.4: Obtain the predictions and display the confusion matrix and the percent of correct predictions here.
percent.correct <- function(x){100*sum(diag(x))/sum(x)}
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.1 <- table(pred.1, Weekly.test$Direction)
x.1
percent.correct(x.1)
## ------------------------------------------------------------------------
##TODO 13.6: Fit a logistic regression model with `Lag1` as the only predictor, and display the confusion matrix and the percent of correct predictions.
lr.all <- glm(Direction ~ Lag1, data = Weekly.train, family = "binomial" )
summary(lr.all)
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.2 <- table(pred.1, Weekly.test$Direction)
x.2
percent.correct(x.2)
## ------------------------------------------------------------------------
##TODO 13.7: Fit a logistic regression model with `Lag2` as the only predictor, and display the confusion matrix and the percent of correct predictions.
lr.all <- glm(Direction ~ Lag2, data = Weekly.train, family = "binomial" )
summary(lr.all)
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.3 <- table(pred.1, Weekly.test$Direction)
x.3
percent.correct(x.3)
## ------------------------------------------------------------------------
##TODO 13.8: Fit a logistic regression model with `Volume` as the only predictor, and display the confusion matrix and the percent of correct predictions.
lr.all <- glm(Direction ~ Volume, data = Weekly.train, family = "binomial" )
summary(lr.all)
pred <- predict(lr.all, Weekly.test, type = "response")
pred.1 <- rep("No", length(pred))
pred.1[pred > 0.5] <- "Yes"
x.4 <- table(pred.1, Weekly.test$Direction)
x.4
percent.correct(x.4)
## ------------------------------------------------------------------------
##TODO 13.10: Fit a k nearest neighbors model with `Lag2` as the only predictor and k=1, and display the confusion matrix and the percent of correct predictions.
library(class)
set.seed(875)
blah <- knn(Weekly.train[3], Weekly.test[3], Weekly.train[,9], k = 1, prob = TRUE)
k.1 <- table(blah, Weekly.test[,9])
k.1
percent.correct(k.1)
Weekly.train[3]
Weekly.train[, 3]
typeof(Weekly.train[3])
typeof(Weekly.train[,3])
?knn
blah <- knn(Weekly.train[,3], Weekly.test[,3], Weekly.train[,9], k = 1, prob = TRUE)
str(Weekly)
is.data.frame(Weekly.train[3])
Weekly.train[3]
## ----tidy = FALSE--------------------------------------------------------
library(ggplot2)
logistic <- function(x){exp(x)/(1 + exp(x))}
ggplot(data.frame(x = c(-6, 6)), aes(x)) +
stat_function(fun = logistic)
## ----message = FALSE-----------------------------------------------------
library(MASS)
head(Pima.tr)
## ------------------------------------------------------------------------
Pima.tr$diabetes <- rep(0, dim(Pima.tr)[1])
Pima.tr$diabetes[Pima.tr$type == "Yes"] <- 1
head(Pima.tr)
## ------------------------------------------------------------------------
diabetes.lr1 <- glm(diabetes ~ glu, data = Pima.tr, family = binomial)
diabetes.lr1
summary(diabetes.lr1)
beta0.lr.1 <- coef(diabetes.lr1)[1]
beta1.lr.1 <- coef(diabetes.lr1)[2]
beta0.lr.1
beta1.lr.1
## ------------------------------------------------------------------------
exp(-5.504 + 0.038*150)/(1 + exp(-5.504 + 0.038*150))
## ----tidy = FALSE--------------------------------------------------------
diabetes.logistic.1 <- function(x){
exp(beta0.lr.1+ beta1.lr.1*x)/(1 + exp(beta0.lr.1+
beta1.lr.1*x))
}
ggplot(Pima.tr, aes(x = glu, y = diabetes)) +
stat_function(fun = diabetes.logistic.1) + geom_point()
## ----tidy = FALSE--------------------------------------------------------
ggplot(Pima.tr, aes(x = glu, y = diabetes)) +
geom_point() +
stat_smooth(method = "glm",
method.args = list(family = "binomial"), se = FALSE)
## ------------------------------------------------------------------------
head(Pima.te)
diabetes.probs.1 <- predict(diabetes.lr1, Pima.te, type = "response")
head(diabetes.probs.1)
## ----tidy = FALSE--------------------------------------------------------
diabetes.predict.1 <- rep("No", dim(Pima.te)[1])
diabetes.predict.1[diabetes.probs.1 > 0.5] <- "Yes"
head(diabetes.predict.1)
table(diabetes.predict.1, Pima.te$type)
length(diabetes.predict.1[diabetes.predict.1 == Pima.te$type])/
dim(Pima.te)[1]
## ------------------------------------------------------------------------
diabetes.lr2 <- glm(diabetes ~ glu + bmi,
data = Pima.tr, family = binomial)
diabetes.lr2
summary(diabetes.lr2)
## ------------------------------------------------------------------------
diabetes.probs.2 <- predict(diabetes.lr2, Pima.te, type = "response")
head(diabetes.probs.2)
diabetes.predict.2 <- rep("No", dim(Pima.te)[1])
diabetes.predict.2[diabetes.probs.2 > 0.5] <- "Yes"
head(diabetes.predict.2)
table(diabetes.predict.2, Pima.te$type)
length(diabetes.predict.2[diabetes.predict.2 == Pima.te$type])/
dim(Pima.te)[1]
## ----tidy = FALSE--------------------------------------------------------
lr.int <- -coef(diabetes.lr2)[1]/coef(diabetes.lr2)[3]
lr.slope <- -coef(diabetes.lr2)[2]/coef(diabetes.lr2)[3]
ggplot(Pima.tr, aes(x = glu, y = bmi)) +
geom_point(aes(color = type)) +
geom_abline(intercept = lr.int, slope = lr.slope)
## ----tidy = FALSE--------------------------------------------------------
data(iris)
head(iris)
str(iris)
ggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width)) +
geom_point(aes(color = Species))
ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width)) +
geom_point(aes(color = Species))
## ------------------------------------------------------------------------
set.seed(321)
selected <- sample(1:150, replace = FALSE, size = 75)
iris.train <- iris[selected,]
iris.test <- iris[-selected,]
## ----warning = FALSE, tidy = FALSE, message = FALSE----------------------
library(VGAM)
iris.lr <- vglm(Species ~ Petal.Width + Petal.Length,
data = iris.train, family = multinomial)
summary(iris.lr)
## ------------------------------------------------------------------------
iris.probs <- predict(iris.lr, iris.test[,c(3,4)], type="response")
head(iris.probs)
## ------------------------------------------------------------------------
which.max(c(2,3,1,5,8,3))
which.max(c(2,20,4,5,9,1,0))
class.predictions <- apply(iris.probs, 1, which.max)
head(class.predictions)
class.predictions[class.predictions == 1] <- levels(iris$Species)[1]
class.predictions[class.predictions == 2] <- levels(iris$Species)[2]
class.predictions[class.predictions == 3] <- levels(iris$Species)[3]
head(class.predictions)
## ------------------------------------------------------------------------
table(class.predictions, iris.test$Species)
## ----tidy = FALSE--------------------------------------------------------
u.knn <- "http://blue.for.msu.edu/FOR875/data/knnExample.csv"
knnExample <- read.csv(u.knn, header=TRUE)
str(knnExample)
ggplot(data = knnExample, aes(x = x1, y = x2)) +
geom_point(aes(color = as.factor(class))) +
theme_bw()
## ------------------------------------------------------------------------
expand.grid(x = c(1,2), y = c(5,3.4,2))
min(knnExample$x1)
max(knnExample$x1)
min(knnExample$x2)
max(knnExample$x2)
x.test <- expand.grid(x1 = seq(-2.6, 4.2, by=0.1), x2 = seq(-2, 2.9, by=0.1))
## ------------------------------------------------------------------------
library(class)
Example_knn <- knn(knnExample[,c(1,2)], x.test, knnExample[,3], k = 15, prob = TRUE)
prob <- attr(Example_knn, "prob")
head(prob)
## ----tidy = TRUE---------------------------------------------------------
library(dplyr)
df1 <- mutate(x.test, prob = prob, class = 0,  prob_cls = ifelse(Example_knn == class, 1, 0))
str(df1)
df2 <- mutate(x.test, prob = prob, class = 1,  prob_cls = ifelse(Example_knn == class, 1, 0))
bigdf <- bind_rows(df1, df2)
names(knnExample)
ggplot(bigdf) +
geom_point(aes(x=x1, y =x2, col=class), data = mutate(x.test, class = Example_knn), size = 0.5) +
geom_point(aes(x = x1, y = x2, col = as.factor(class)), size = 4, shape = 1, data = knnExample) +
geom_contour(aes(x = x1, y = x2, z = prob_cls, group = as.factor(class), color = as.factor(class)), size = 1, bins = 1, data = bigdf) + theme_bw()
## ----warning=FALSE-------------------------------------------------------
Example_knn <- knn(knnExample[,c(1,2)], x.test, knnExample[,3], k = 1, prob = TRUE)
prob <- attr(Example_knn, "prob")
head(prob)
df1 <- mutate(x.test, prob = prob, class = 0,  prob_cls = ifelse(Example_knn == class, 1, 0))
str(df1)
df2 <- mutate(x.test, prob = prob, class = 1,  prob_cls = ifelse(Example_knn == class, 1, 0))
bigdf <- bind_rows(df1, df2)
ggplot(bigdf) + geom_point(aes(x = x1, y = x2, col = class), data = mutate(x.test, class = Example_knn), size = 0.5) + geom_point(aes(x = x1, y = x2, col = as.factor(class)), size = 4, shape = 1, data = knnExample) + geom_contour(aes(x = x1, y = x2, z = prob_cls, group = as.factor(class), color = as.factor(class)), size = 1, bins = 1, data = bigdf) + theme_bw()
## ------------------------------------------------------------------------
Pima.tr[,1:7] <- scale(Pima.tr[,1:7], center = TRUE, scale = TRUE)
Pima.te[,1:7] <- scale(Pima.te[,1:7], center = TRUE, scale = TRUE)
knn_Pima <- knn(Pima.tr[,c(2,5)], Pima.te[,c(2,5)], Pima.tr[,8], k = 15, prob=TRUE)
table(knn_Pima, Pima.te[,8])
## ------------------------------------------------------------------------
sd(iris.train$Petal.Width)
sd(iris.train$Petal.Length)
head(iris.train)
knn_iris <- knn(iris.train[,c(3,4)], iris.test[,c(3,4)], iris.train[,5], k=1, prob=TRUE)
table(knn_iris, iris.test[,5])
knn_iris <- knn(iris.train[,c(3,4)], iris.test[,c(3,4)], iris.train[,5], k=3, prob=TRUE)
table(knn_iris, iris.test[,5])
knn_iris <- knn(iris.train[,c(3,4)], iris.test[,c(3,4)], iris.train[,5], k=15, prob=TRUE)
table(knn_iris, iris.test[,5])
iris.train[, c(3, 4)]
is.data.frame(iris.train[, c(3, 4)])
is.data.frame(Pima.tr[, c(2, 5)])
str(Weekly.train)
Weekly.train[, 9]
Weekly.train[, 8]
Pima.tr[, c(2, 5)]
